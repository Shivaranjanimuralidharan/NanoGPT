Module 9 â€“ Deep Generative Models

ðŸ”¸ Variational Autoencoder (VAE)
- Learns structured latent space
- Can generate new samples from learned distribution
- Adds probabilistic interpretation to latent variables
- Encoder learns mean and variance of latent distribution
- Decoder reconstructs input from latent code
- Loss = Reconstruction loss + KL divergence

ðŸ”¸ Normalizing Flows
- Method to transform simple distribution into complex one
- Uses invertible neural networks
- Helps improve expressiveness of latent space

ðŸ”¸ Restricted Boltzmann Machines (RBM)
- Unsupervised probabilistic generative model
- Energy-based model with visible and hidden units
- Learns data distribution using Gibbs Sampling
- Contrastive divergence for training
- Used for dimensionality reduction and pretraining deep networks

ðŸ”¸ Deep Belief Networks (DBN)
- Stack of RBMs
- Layer-wise unsupervised training
- Used to initialize deep architectures
- Later fine-tuned using supervised learning

ðŸ”¸ Limitations of RBM/DBN
- Slow training
- Poor generalization
- Often replaced by VAE, GAN, Transformers in modern models


















